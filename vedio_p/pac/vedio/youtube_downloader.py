import os
import subprocess
import time
import random
import logging
import sys
import concurrent.futures
import re
from datetime import datetime
from urllib.parse import urlparse, parse_qs
from tqdm import tqdm

# é…ç½®æ—¥å¿—
def setup_logging(log_file):
    log_dir = os.path.dirname(log_file)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        force=True
    )
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    console_handler.setLevel(logging.INFO)
    console_handler.stream = open(sys.stdout.fileno(), mode='w', encoding='utf-8')
    logging.getLogger().addHandler(console_handler)

# å®‰å…¨æ–‡ä»¶åå¤„ç†
def sanitize_filename(filename):
    invalid_chars = r'[\\/*?:"<>|]'
    return re.sub(invalid_chars, '_', filename)

# åŠ¨æ€ä»£ç†é…ç½®
def set_proxy(proxy):
    if proxy:
        os.environ["HTTP_PROXY"] = proxy
        os.environ["HTTPS_PROXY"] = proxy

# ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼Œé¿å…æ–‡ä»¶è¦†ç›–
def generate_unique_filename(title, ext):
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    safe_title = sanitize_filename(title)
    return f"{safe_title}_{timestamp}.{ext}"

# ä¸‹è½½ YouTube è§†é¢‘
def download_youtube_video(url, output_path, keep_original, retries=0, proxy=None):
    video_id = parse_qs(urlparse(url).query).get('v', [None])[0]
    if not video_id:
        logging.error(f"âŒ æ— æ•ˆçš„è§†é¢‘é“¾æ¥: {url}")
        return False

    # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡
    for file in os.listdir(output_path):
        if video_id in file:
            logging.info(f"ğŸ“Œ è§†é¢‘ {url} å·²ä¸‹è½½ï¼Œè·³è¿‡ã€‚")
            return True

    max_retries = 3
    backoff_base = 2
    while retries < max_retries:
        try:
            command = [
                "yt-dlp",
                "-f", "bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4]",
                "--merge-output-format", "mp4",
                "--write-thumbnail",
                "--add-metadata",
                "-o", os.path.join(output_path, generate_unique_filename("%(title)s", "%(ext)s")),
                "--progress",  # å¯ç”¨è¿›åº¦ä¿¡æ¯è¾“å‡º
                "--continue",  # å¼€å¯æ–­ç‚¹ç»­ä¼ 
                "-v"
            ]

            if proxy:
                command.extend(["--proxy", proxy])
            if keep_original:
                command.append("-k")

            command.append(url)

            logging.info(f"å¼€å§‹ä¸‹è½½: {url}")
            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8')

            download_pbar = None
            merge_pbar = None
            for line in process.stdout:
                if "[download]" in line:
                    parts = line.split()
                    if len(parts) >= 6:
                        try:
                            percent = float(parts[1].strip('%'))
                            speed = parts[3]
                            eta = parts[5]
                            if download_pbar is None:
                                download_pbar = tqdm(total=100, desc=f"ä¸‹è½½ {url}", unit="%", ncols=100)
                            download_pbar.n = percent
                            # è§£æé€Ÿåº¦ä¿¡æ¯ï¼Œè½¬æ¢ä¸ºç»Ÿä¸€çš„å¯è¯»æ ¼å¼
                            if speed.endswith('KiB/s'):
                                speed_value = float(speed[:-5])
                                speed_str = f"{speed_value:.2f} KB/s"
                            elif speed.endswith('MiB/s'):
                                speed_value = float(speed[:-5])
                                speed_str = f"{speed_value:.2f} MB/s"
                            else:
                                speed_str = speed
                            download_pbar.set_postfix({"é€Ÿåº¦": speed_str, "å‰©ä½™æ—¶é—´": eta})
                            download_pbar.refresh()
                        except ValueError:
                            pass
                elif "[Merger]" in line:
                    parts = line.split()
                    if len(parts) >= 4:
                        try:
                            merge_percent = float(parts[2].strip('%'))
                            if merge_pbar is None:
                                merge_pbar = tqdm(total=100, desc=f"åˆæˆ {url}", unit="%", ncols=100)
                            merge_pbar.n = merge_percent
                            merge_pbar.refresh()
                        except ValueError:
                            pass

            if download_pbar:
                download_pbar.close()
            if merge_pbar:
                merge_pbar.close()

            process.wait()
            if process.returncode == 0:
                logging.info(f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆï¼{url}")
                return True
            else:
                if "HTTP Error 429" in process.stdout.read():
                    logging.warning(f"âš ï¸ ä¸‹è½½ {url} æ—¶é‡åˆ° 429 é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•...")
                else:
                    logging.error(f"âš ï¸ è§†é¢‘ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼{url}")
                    logging.error(f"é”™è¯¯ä¿¡æ¯: {process.stdout.read()}")
        except Exception as e:
            logging.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

        wait_time = random.uniform(backoff_base ** retries, backoff_base ** (retries + 1))
        logging.info(f"â³ ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•... (ç¬¬ {retries + 1} æ¬¡é‡è¯•)")
        time.sleep(wait_time)
        retries += 1

    logging.error(f"âŒ ä¸‹è½½ {url} å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚")
    return False

# ä» txt æ–‡ä»¶ä¸­æå– YouTube è§†é¢‘é“¾æ¥
def extract_video_links_from_txt(txt_file):
    links = []
    try:
        with open(txt_file, "r", encoding="utf-8") as f:
            links = [line.strip() for line in f.readlines() if line.strip()]

        if not links:
            logging.warning("æœªä» txt æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆçš„è§†é¢‘é“¾æ¥ï¼")

        return links

    except Exception as e:
        logging.error(f"è¯»å– txt æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

# è·å– YouTube è§†é¢‘é“¾æ¥
def get_youtube_video_links(query, output_file, max_results=10, proxy=None):
    try:
        command = [
            "yt-dlp",
            "--proxy", proxy if proxy else "http://127.0.0.1:7890",
            f"ytsearch{max_results}:{query}",
            "--get-id"
        ]
        result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')

        if result.returncode == 0:
            video_ids = result.stdout.strip().split("\n")
            if not video_ids:
                logging.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°ä»»ä½•è§†é¢‘é“¾æ¥ï¼")
                return []

            video_links = [f"https://www.youtube.com/watch?v={video_id}" for video_id in video_ids]

            output_dir = os.path.dirname(output_file)
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)

            mode = "a" if os.path.exists(output_file) else "w"
            with open(output_file, mode, encoding="utf-8") as f:
                f.write("\n".join(video_links) + "\n")

            logging.info(f"âœ… æˆåŠŸè·å– {len(video_links)} ä¸ªè§†é¢‘é“¾æ¥ï¼")
            return video_links
        else:
            logging.error("âš ï¸ è·å–è§†é¢‘é“¾æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼")
            logging.error(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return []

    except Exception as e:
        logging.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        return []

if __name__ == "__main__":
    default_output_path = r"D:\ç´ èœåº“\youtube\vedio\result"
    OUTPUT_PATH = input(f"è¯·è¾“å…¥è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤ {default_output_path}ï¼‰ï¼š")
    if not OUTPUT_PATH:
        OUTPUT_PATH = default_output_path
    os.makedirs(OUTPUT_PATH, exist_ok=True)

    default_log_file = r"D:\pythonProject\vedio_p\pac\vedio\logs\download_log.txt"
    LOG_FILE = input(f"è¯·è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ {default_log_file}ï¼‰ï¼š")
    if not LOG_FILE:
        LOG_FILE = default_log_file

    KEEP_ORIGINAL_FILES = input("æ˜¯å¦ä¿ç•™åŸå§‹æ–‡ä»¶ï¼Ÿ(y/n, é»˜è®¤ n)ï¼š").lower() == 'y'
    PROXY = input(f"è¯·è¾“å…¥ä»£ç†è®¾ç½®ï¼ˆé»˜è®¤ {r'http://127.0.0.1:7890'}ï¼‰ï¼š")
    if not PROXY:
        PROXY = r'http://127.0.0.1:7890'

    CONCURRENCY = input("è¯·è¾“å…¥å¹¶å‘ä¸‹è½½æ•°é‡ï¼ˆé»˜è®¤ 3ï¼‰ï¼š")
    try:
        CONCURRENCY = int(CONCURRENCY) if CONCURRENCY else 3
    except ValueError:
        CONCURRENCY = 3

    setup_logging(LOG_FILE)
    set_proxy(PROXY)

    while True:
        choice = input("è¯·é€‰æ‹©æ“ä½œæ–¹å¼ï¼š1. å…³é”®è¯æœç´¢ä¸‹è½½ 2. ç›´æ¥è¾“å…¥é“¾æ¥ä¸‹è½½ï¼š")
        if choice == '1':
            SEARCH_QUERY = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼š")
            while True:
                try:
                    MAX_RESULTS = int(input("è¯·è¾“å…¥æœ€å¤§æœç´¢ç»“æœæ•°é‡ï¼š"))
                    break
                except ValueError:
                    print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ•´æ•°ã€‚")

            output_file_name = f"youtube_{SEARCH_QUERY.replace(' ', '_')}_links.txt"
            OUTPUT_FILE = os.path.join(r"D:\pythonProject\vedio_p\pac\vedio\link", output_file_name)

            links = get_youtube_video_links(SEARCH_QUERY, OUTPUT_FILE, max_results=MAX_RESULTS, proxy=PROXY)
            video_links = extract_video_links_from_txt(OUTPUT_FILE)
            break
        elif choice == '2':
            input_links = input("è¯·è¾“å…¥è¦ä¸‹è½½çš„ YouTube è§†é¢‘é“¾æ¥ï¼Œå¤šä¸ªé“¾æ¥ç”¨é€—å·åˆ†éš”ï¼š").split(',')
            video_links = [link.strip() for link in input_links]
            break
        else:
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ 1 æˆ– 2ã€‚")

    if video_links:
        logging.info(f"å‡†å¤‡ä¸‹è½½ {len(video_links)} ä¸ªè§†é¢‘é“¾æ¥")
        success_count = 0
        failed_count = 0

        with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
            future_to_url = {executor.submit(download_youtube_video, url, OUTPUT_PATH, KEEP_ORIGINAL_FILES, proxy=PROXY): url for url in video_links}
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        success_count += 1
                    else:
                        failed_count += 1
                except Exception as exc:
                    logging.error(f'%r generated an exception: %s' % (url, exc))
                    failed_count += 1

        logging.info(f"ä¸‹è½½ç»Ÿè®¡ï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªã€‚")
    else:
        logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘é“¾æ¥ï¼Œæ— æ³•è¿›è¡Œä¸‹è½½ã€‚")